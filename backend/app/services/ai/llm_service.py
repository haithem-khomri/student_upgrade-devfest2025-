"""
LLM Service - Provider-agnostic abstraction for LLM interactions

CONFIGURATION:
1. Set LLM_PROVIDER in .env (options: "openai", "anthropic", "google", "custom")
2. Add the appropriate API key

Supported Providers:
- OpenAI: Set OPENAI_API_KEY
- Anthropic: Set ANTHROPIC_API_KEY  
- Google Gemini: Set GOOGLE_API_KEY
- Custom: Set CUSTOM_LLM_API_URL and CUSTOM_LLM_API_KEY
"""
from typing import Dict, Optional, List, Any
import httpx
import json
from abc import ABC, abstractmethod

from app.core.config import settings


class BaseLLMProvider(ABC):
    """Base class for LLM providers"""
    
    @abstractmethod
    async def complete(
        self,
        message: str,
        system_prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> str:
        pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAI GPT Provider"""
    
    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):
        self.api_key = api_key
        self.model = model
    
    async def complete(
        self,
        message: str,
        system_prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> str:
        try:
            from openai import AsyncOpenAI
            client = AsyncOpenAI(api_key=self.api_key)
            
            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": message},
                ],
                max_tokens=max_tokens,
                temperature=temperature,
            )
            
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f"OpenAI API error: {e}")


class AnthropicProvider(BaseLLMProvider):
    """Anthropic Claude Provider"""
    
    def __init__(self, api_key: str, model: str = "claude-3-sonnet-20240229"):
        self.api_key = api_key
        self.model = model
    
    async def complete(
        self,
        message: str,
        system_prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> str:
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "https://api.anthropic.com/v1/messages",
                    headers={
                        "x-api-key": self.api_key,
                        "anthropic-version": "2023-06-01",
                        "content-type": "application/json",
                    },
                    json={
                        "model": self.model,
                        "max_tokens": max_tokens,
                        "system": system_prompt,
                        "messages": [{"role": "user", "content": message}],
                    },
                    timeout=30.0,
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data["content"][0]["text"]
                else:
                    raise Exception(f"Anthropic API error: {response.status_code}")
        except Exception as e:
            raise Exception(f"Anthropic API error: {e}")


class GoogleGeminiProvider(BaseLLMProvider):
    """Google Gemini Provider - Using official SDK"""
    
    def __init__(self, api_key: str, model: str = "gemini-1.5-flash"):
        if not api_key:
            raise ValueError("GOOGLE_API_KEY is required for Gemini provider")
        self.api_key = api_key
        self.model = model
        self._configured = False
    
    def _configure(self):
        """Configure the Gemini API"""
        if not self._configured:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            self._configured = True
            print(f"[Gemini] Configured with model: {self.model}")
    
    async def complete(
        self,
        message: str,
        system_prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> str:
        try:
            import google.generativeai as genai
            import asyncio
            
            # Configure the API key
            self._configure()
            
            # Use the configured model (fallback to gemini-1.5-flash if invalid)
            model_name = self.model
            # Try to use the configured model, fallback to available models
            try:
                model = genai.GenerativeModel(
                    model_name,
                    generation_config={
                        "max_output_tokens": max_tokens,
                        "temperature": temperature,
                    }
                )
            except Exception as model_error:
                print(f"[Gemini] Warning: Model {model_name} not available, trying gemini-1.5-flash")
                # Fallback to gemini-1.5-flash if the configured model doesn't exist
                model_name = "gemini-1.5-flash"
                model = genai.GenerativeModel(
                    model_name,
                    generation_config={
                        "max_output_tokens": max_tokens,
                        "temperature": temperature,
                    }
                )
            
            # Combine system prompt and message
            full_prompt = f"{system_prompt}\n\nUser: {message}"
            
            print(f"[Gemini] Calling API with model: {model_name}, message length: {len(message)}")
            
            # Generate content in executor for async compatibility
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                lambda: model.generate_content(full_prompt)
            )
            
            if not response or not response.text:
                raise Exception("Empty response from Gemini API")
            
            print(f"[Gemini] Successfully received response (length: {len(response.text)})")
            return response.text
        except Exception as e:
            error_msg = f"Google Gemini API error: {str(e)}"
            print(f"[Gemini] Error: {error_msg}")
            raise Exception(error_msg)


class CustomLLMProvider(BaseLLMProvider):
    """
    Custom LLM Provider - Use this to integrate your own LLM API
    
    Configure:
    - CUSTOM_LLM_API_URL: Your API endpoint
    - CUSTOM_LLM_API_KEY: Your API key
    
    Expected API format (adjust _build_request and _parse_response as needed):
    POST to your endpoint with JSON body containing the message
    """
    
    def __init__(self, api_url: str, api_key: str):
        self.api_url = api_url
        self.api_key = api_key
    
    def _build_request(
        self,
        message: str,
        system_prompt: str,
        max_tokens: int,
        temperature: float,
    ) -> Dict[str, Any]:
        """
        Build the request body for your custom LLM API
        
        MODIFY THIS METHOD to match your API's expected format
        """
        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": message},
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
    
    def _build_headers(self) -> Dict[str, str]:
        """
        Build request headers for your custom LLM API
        
        MODIFY THIS METHOD to match your API's authentication
        """
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
        }
    
    def _parse_response(self, data: Dict[str, Any]) -> str:
        """
        Parse the response from your custom LLM API
        
        MODIFY THIS METHOD to extract the response text from your API's format
        """
        # Common formats - adjust based on your API
        if "choices" in data:
            # OpenAI-compatible format
            return data["choices"][0]["message"]["content"]
        elif "response" in data:
            return data["response"]
        elif "text" in data:
            return data["text"]
        elif "content" in data:
            if isinstance(data["content"], list):
                return data["content"][0]["text"]
            return data["content"]
        elif "message" in data:
            return data["message"]
        else:
            # Return the whole data as string if format is unknown
            return str(data)
    
    async def complete(
        self,
        message: str,
        system_prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> str:
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    self.api_url,
                    headers=self._build_headers(),
                    json=self._build_request(message, system_prompt, max_tokens, temperature),
                    timeout=60.0,  # Longer timeout for custom APIs
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return self._parse_response(data)
                else:
                    raise Exception(f"Custom LLM API error: {response.status_code} - {response.text}")
        except Exception as e:
            raise Exception(f"Custom LLM API error: {e}")


class LLMService:
    """
    Main LLM Service - Provider-agnostic wrapper
    
    Usage:
        llm = LLMService()
        response = await llm.chat_completion("Hello!", context={}, language="ar")
    """
    
    def __init__(self):
        self.provider = settings.LLM_PROVIDER
        self._llm_provider = self._get_provider()
    
    def _get_provider(self) -> Optional[BaseLLMProvider]:
        """Get the configured LLM provider"""
        if self.provider == "openai" and settings.OPENAI_API_KEY:
            print(f"[LLMService] Initializing OpenAI provider with model: {getattr(settings, 'OPENAI_MODEL', 'gpt-3.5-turbo')}")
            return OpenAIProvider(
                api_key=settings.OPENAI_API_KEY,
                model=getattr(settings, 'OPENAI_MODEL', 'gpt-3.5-turbo'),
            )
        elif self.provider == "anthropic" and settings.ANTHROPIC_API_KEY:
            print(f"[LLMService] Initializing Anthropic provider with model: {getattr(settings, 'ANTHROPIC_MODEL', 'claude-3-sonnet-20240229')}")
            return AnthropicProvider(
                api_key=settings.ANTHROPIC_API_KEY,
                model=getattr(settings, 'ANTHROPIC_MODEL', 'claude-3-sonnet-20240229'),
            )
        elif self.provider == "google":
            google_api_key = getattr(settings, 'GOOGLE_API_KEY', None)
            if google_api_key:
                google_model = getattr(settings, 'GOOGLE_MODEL', 'gemini-1.5-flash')
                print(f"[LLMService] Initializing Google Gemini provider with model: {google_model}")
                try:
                    return GoogleGeminiProvider(
                        api_key=google_api_key,
                        model=google_model,
                    )
                except Exception as e:
                    print(f"[LLMService] Error initializing Gemini provider: {e}")
                    return None
            else:
                print("[LLMService] Warning: LLM_PROVIDER is 'google' but GOOGLE_API_KEY is not set")
                return None
        elif self.provider == "custom":
            custom_url = getattr(settings, 'CUSTOM_LLM_API_URL', None)
            custom_key = getattr(settings, 'CUSTOM_LLM_API_KEY', None)
            if custom_url and custom_key:
                print(f"[LLMService] Initializing Custom LLM provider with URL: {custom_url}")
                return CustomLLMProvider(api_url=custom_url, api_key=custom_key)
        
        print(f"[LLMService] No LLM provider configured (provider: {self.provider}), using fallback responses")
        return None
    
    async def chat_completion(
        self,
        message: str,
        context: Optional[Dict] = None,
        language: str = "ar",
        short_answer: bool = False,
    ) -> str:
        """
        Get chat completion from configured LLM
        
        Args:
            message: User message
            context: Additional context (user level, modules, etc.)
            language: Language code ('en', 'ar', 'fr')
            short_answer: Whether to return short answer (for mobile)
        """
        system_prompt = self._build_system_prompt(context, language, short_answer)
        max_tokens = 500 if short_answer else 1000
        
        if self._llm_provider:
            try:
                print(f"[LLMService] Calling {self.provider} provider for message: {message[:50]}...")
                response = await self._llm_provider.complete(
                    message=message,
                    system_prompt=system_prompt,
                    max_tokens=max_tokens,
                    temperature=0.7,
                )
                print(f"[LLMService] Successfully received response from {self.provider} (length: {len(response)})")
                return response
            except Exception as e:
                print(f"[LLMService] Error calling {self.provider} provider: {e}")
                print(f"[LLMService] Falling back to predefined responses")
                return self._fallback_response(message, language)
        else:
            print(f"[LLMService] No provider configured, using fallback response")
            return self._fallback_response(message, language)
    
    def _build_system_prompt(
        self,
        context: Optional[Dict],
        language: str,
        short_answer: bool,
    ) -> str:
        """Build system prompt based on context"""
        if language == "ar":
            prompt = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯ Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª. Ø£Ù†Øª Ù…ØªØ®ØµØµ ÙÙŠ:
- Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø·Ù„Ø§Ø¨ Ø­ÙˆÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³Ø© ÙˆØ§Ù„Ø¬Ø§Ù…Ø¹Ø©
- ØªÙ‚Ø¯ÙŠÙ… Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø¯Ø±Ø§Ø³Ø© ÙˆØªÙ†Ø¸ÙŠÙ… Ø§Ù„ÙˆÙ‚Øª
- Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø§Ù„ØªØ­Ø¶ÙŠØ± Ù„Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª
- ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø·Ù„Ø§Ø¨ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ

ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙˆÙ…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹. Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙØµÙŠØ­Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„ÙÙ‡Ù…."""
        else:
            prompt = "You are a helpful AI assistant for university students. "
        
        if context:
            if context.get("user_level"):
                if language == "ar":
                    prompt += f"\nÙ…Ø³ØªÙˆÙ‰ Ø§Ù„Ø·Ø§Ù„Ø¨: {context['user_level']}. "
                else:
                    prompt += f"The student is at level {context['user_level']}. "
            if context.get("user_modules"):
                if language == "ar":
                    prompt += f"\nØ§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØªÙŠ ÙŠØ¯Ø±Ø³Ù‡Ø§: {', '.join(context['user_modules'])}. "
                else:
                    prompt += f"They are studying: {', '.join(context['user_modules'])}. "
        
        if short_answer:
            if language == "ar":
                prompt += "\nØ£Ø¹Ø·Ù Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø®ØªØµØ±Ø© ÙˆÙ…ÙÙŠØ¯Ø© (2-3 Ø¬Ù…Ù„ ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)."
            else:
                prompt += "Keep your response brief and concise (2-3 sentences maximum). "
        
        return prompt
    
    def _fallback_response(self, message: str, language: str) -> str:
        """Smart fallback response when LLM is unavailable"""
        message_lower = message.strip().lower()
        
        # Keyword-based responses for Arabic
        if language == "ar":
            # Study tips
            if any(word in message for word in ['Ø§Ù…ØªØ­Ø§Ù†', 'Ø§Ø®ØªØ¨Ø§Ø±', 'Ø¯Ø±Ø§Ø³Ø©', 'Ù…Ø°Ø§ÙƒØ±Ø©', 'Ø§Ø³ØªØ¹Ø¯Ø§Ø¯']):
                return """Ø¥Ù„ÙŠÙƒ Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ù„Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª:

ðŸ“š **Ø§Ù„ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø³Ø¨Ù‚:**
â€¢ Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø¨Ù„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
â€¢ Ù‚Ø³Ù‘Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ ØµØºÙŠØ±Ø©
â€¢ Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ù„Ø®ØµØ§Øª ÙˆØ§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

â° **ØªÙ†Ø¸ÙŠÙ… Ø§Ù„ÙˆÙ‚Øª:**
â€¢ Ø®ØµØµ 45 Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„Ø¯Ø±Ø§Ø³Ø© Ø«Ù… 10 Ø¯Ù‚Ø§Ø¦Ù‚ Ø±Ø§Ø­Ø©
â€¢ Ø§Ø¯Ø±Ø³ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØµØ¹Ø¨Ø© ÙÙŠ Ø£ÙˆÙ‚Ø§Øª Ù†Ø´Ø§Ø·Ùƒ
â€¢ Ù„Ø§ ØªØ³Ù‡Ø± Ù„ÙŠÙ„Ø© Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†

ðŸ’¡ **Ù†ØµØ§Ø¦Ø­ Ø¥Ø¶Ø§ÙÙŠØ©:**
â€¢ Ø­Ù„ Ø§Ù…ØªØ­Ø§Ù†Ø§Øª Ø³Ø§Ø¨Ù‚Ø©
â€¢ Ø§Ø´Ø±Ø­ Ø§Ù„Ù…Ø§Ø¯Ø© Ù„Ø´Ø®Øµ Ø¢Ø®Ø±
â€¢ Ù†Ù… Ø¬ÙŠØ¯Ø§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†

Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚! ðŸŒŸ"""
            
            # Time management
            elif any(word in message for word in ['ÙˆÙ‚Øª', 'ØªÙ†Ø¸ÙŠÙ…', 'Ø¬Ø¯ÙˆÙ„', 'Ø¥Ø¯Ø§Ø±Ø©']):
                return """Ø¥Ù„ÙŠÙƒ Ø®Ø·ÙˆØ§Øª Ù„ØªÙ†Ø¸ÙŠÙ… ÙˆÙ‚ØªÙƒ Ø¨ÙØ¹Ø§Ù„ÙŠØ©:

ðŸ“‹ **Ø§Ù„ØªØ®Ø·ÙŠØ·:**
â€¢ Ø§ÙƒØªØ¨ ÙƒÙ„ Ù…Ù‡Ø§Ù…Ùƒ Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
â€¢ Ø­Ø¯Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª (Ù…Ù‡Ù…/Ø¹Ø§Ø¬Ù„)
â€¢ Ø§Ø³ØªØ®Ø¯Ù… ØªØ·Ø¨ÙŠÙ‚ ØªÙ†Ø¸ÙŠÙ… Ø£Ùˆ Ø¯ÙØªØ±

â±ï¸ **ØªÙ‚Ù†ÙŠØ§Øª ÙØ¹Ø§Ù„Ø©:**
â€¢ ØªÙ‚Ù†ÙŠØ© Pomodoro: 25 Ø¯Ù‚ÙŠÙ‚Ø© Ø¹Ù…Ù„ + 5 Ø±Ø§Ø­Ø©
â€¢ Ø®ØµØµ Ø£ÙˆÙ‚Ø§Øª Ø«Ø§Ø¨ØªØ© Ù„Ù„Ø¯Ø±Ø§Ø³Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹
â€¢ ØªØ¬Ù†Ø¨ ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù‡Ø§Ù…

ðŸŽ¯ **Ù†ØµØ§Ø¦Ø­:**
â€¢ Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØµØ¹Ø¨Ø© ØµØ¨Ø§Ø­Ø§Ù‹
â€¢ Ø®ØµØµ ÙˆÙ‚Øª Ù„Ù„Ø±Ø§Ø­Ø© ÙˆØ§Ù„ØªØ±ÙÙŠÙ‡
â€¢ Ø±Ø§Ø¬Ø¹ Ø¬Ø¯ÙˆÙ„Ùƒ Ø£Ø³Ø¨ÙˆØ¹ÙŠØ§Ù‹

Ø§Ù„Ù†Ø¬Ø§Ø­ ÙŠØ¨Ø¯Ø£ Ø¨Ø§Ù„ØªÙ†Ø¸ÙŠÙ…! ðŸ’ª"""
            
            # Focus and concentration
            elif any(word in message for word in ['ØªØ±ÙƒÙŠØ²', 'Ø§Ù†ØªØ¨Ø§Ù‡', 'ØªØ´ØªØª', 'Ù…Ù„Ù„']):
                return """Ø¥Ù„ÙŠÙƒ Ù†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† ØªØ±ÙƒÙŠØ²Ùƒ:

ðŸŽ¯ **Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©:**
â€¢ Ø§Ø®ØªØ± Ù…ÙƒØ§Ù† Ù‡Ø§Ø¯Ø¦ ÙˆÙ…Ø±ØªØ¨
â€¢ Ø£Ø¨Ø¹Ø¯ Ø§Ù„Ù‡Ø§ØªÙ Ø£Ùˆ ÙØ¹Ù‘Ù„ ÙˆØ¶Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ²
â€¢ ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§Ø¡Ø© Ø¬ÙŠØ¯Ø©

ðŸ§  **ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ±ÙƒÙŠØ²:**
â€¢ Ø§Ø¨Ø¯Ø£ Ø¨Ù€ 25 Ø¯Ù‚ÙŠÙ‚Ø© ÙÙ‚Ø· Ø«Ù… Ø²ÙØ¯ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹
â€¢ Ø§Ø³ØªØ®Ø¯Ù… Ø³Ù…Ø§Ø¹Ø§Øª Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
â€¢ Ø§ÙƒØªØ¨ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ù…Ø´ØªØªØ© ÙˆØ£Ø¬Ù‘Ù„Ù‡Ø§

ðŸ’§ **Ø§Ù„Ø¹Ù†Ø§ÙŠØ© Ø¨Ø§Ù„Ø¬Ø³Ù…:**
â€¢ Ø§Ø´Ø±Ø¨ Ù…Ø§Ø¡ ÙƒØ§ÙÙŠ
â€¢ ØªÙ†Ø§ÙˆÙ„ ÙˆØ¬Ø¨Ø§Øª Ø®ÙÙŠÙØ© ØµØ­ÙŠØ©
â€¢ Ø®Ø° ÙØªØ±Ø§Øª Ø±Ø§Ø­Ø© Ù‚ØµÙŠØ±Ø©

Ø§Ù„ØªØ±ÙƒÙŠØ² Ù…Ù‡Ø§Ø±Ø© ØªØªØ­Ø³Ù† Ø¨Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø©! ðŸŒŸ"""
            
            # Greeting
            elif any(word in message for word in ['Ù…Ø±Ø­Ø¨Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù…', 'Ø£Ù‡Ù„Ø§', 'Ù‡Ù„Ø§', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡']):
                return """Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ! ðŸ‘‹

Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ø¯Ø±Ø§Ø³Ø©. ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ:

ðŸ“š **Ø§Ù„Ø¯Ø±Ø§Ø³Ø© ÙˆØ§Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª** - Ù†ØµØ§Ø¦Ø­ Ù„Ù„ØªØ­Ø¶ÙŠØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©
â° **ØªÙ†Ø¸ÙŠÙ… Ø§Ù„ÙˆÙ‚Øª** - Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØªÙ‚Ù†ÙŠØ§Øª ÙØ¹Ø§Ù„Ø©  
ðŸŽ¯ **Ø§Ù„ØªØ±ÙƒÙŠØ²** - Ø·Ø±Ù‚ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
ðŸ“– **Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©** - Ø´Ø±Ø­ ÙˆÙ…Ø³Ø§Ø¹Ø¯Ø©

ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ ðŸ˜Š"""
            
            # Default Arabic response
            else:
                return f"""Ø´ÙƒØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ! ðŸ˜Š

Ø³Ø£Ø­Ø§ÙˆÙ„ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù†. Ø³Ø¤Ø§Ù„Ùƒ: "{message[:100]}..."

ðŸ’¡ **Ù†ØµÙŠØ­Ø© Ø³Ø±ÙŠØ¹Ø©:**
Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ø¬Ø±Ø¨ Ø£Ø³Ø¦Ù„Ø© Ù…Ø­Ø¯Ø¯Ø© Ù…Ø«Ù„:
â€¢ ÙƒÙŠÙ Ø£Ø³ØªØ¹Ø¯ Ù„Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§ØªØŸ
â€¢ Ø£Ø±ÙŠØ¯ Ù†ØµØ§Ø¦Ø­ Ù„Ù„ØªØ±ÙƒÙŠØ²
â€¢ ÙƒÙŠÙ Ø£Ù†Ø¸Ù… ÙˆÙ‚ØªÙŠØŸ

Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ! ðŸŒŸ"""
        
        # English responses
        else:
            if any(word in message_lower for word in ['exam', 'test', 'study', 'prepare']):
                return """Here are some exam preparation tips:

ðŸ“š **Preparation:**
â€¢ Start reviewing at least 2 weeks early
â€¢ Break material into smaller chunks
â€¢ Focus on key concepts and summaries

â° **Time Management:**
â€¢ Study for 45 min, then take 10 min breaks
â€¢ Study difficult subjects during peak energy
â€¢ Don't pull all-nighters before exams

ðŸ’¡ **Extra Tips:**
â€¢ Practice with past exams
â€¢ Teach the material to someone else
â€¢ Get good sleep before the exam

Good luck! ðŸŒŸ"""
            
            elif any(word in message_lower for word in ['time', 'schedule', 'organize', 'manage']):
                return """Here's how to manage your time effectively:

ðŸ“‹ **Planning:**
â€¢ Write down all your daily tasks
â€¢ Prioritize (important/urgent)
â€¢ Use a planner app or notebook

â±ï¸ **Effective Techniques:**
â€¢ Pomodoro: 25 min work + 5 min break
â€¢ Set fixed daily study times
â€¢ Avoid multitasking

ðŸŽ¯ **Tips:**
â€¢ Tackle hard tasks in the morning
â€¢ Schedule time for rest and fun
â€¢ Review your schedule weekly

Success starts with organization! ðŸ’ª"""
            
            elif any(word in message_lower for word in ['focus', 'concentrate', 'distract', 'attention']):
                return """Here are tips to improve your focus:

ðŸŽ¯ **Ideal Environment:**
â€¢ Choose a quiet, tidy space
â€¢ Put your phone away or use focus mode
â€¢ Ensure good lighting

ðŸ§  **Focus Techniques:**
â€¢ Start with just 25 minutes, then increase
â€¢ Use noise-canceling headphones
â€¢ Write down distracting thoughts for later

ðŸ’§ **Self-Care:**
â€¢ Stay hydrated
â€¢ Eat healthy snacks
â€¢ Take short breaks

Focus is a skill that improves with practice! ðŸŒŸ"""
            
            elif any(word in message_lower for word in ['hello', 'hi', 'hey', 'good morning', 'good evening']):
                return """Hello! ðŸ‘‹

I'm your AI study assistant. I can help you with:

ðŸ“š **Study & Exams** - Preparation and review tips
â° **Time Management** - Schedules and techniques
ðŸŽ¯ **Focus** - Ways to improve concentration
ðŸ“– **Coursework** - Explanations and guidance

How can I help you today? ðŸ˜Š"""
            
            else:
                return f"""Thanks for your question! ðŸ˜Š

I'll try to help you as best as I can. Your question: "{message[:100]}..."

ðŸ’¡ **Quick Tip:**
For the best answers, try specific questions like:
â€¢ How do I prepare for exams?
â€¢ I need tips for focusing
â€¢ How do I manage my time?

I'm here to help! ðŸŒŸ"""


# Singleton instance
_llm_service: Optional[LLMService] = None

def get_llm_service() -> LLMService:
    """Get or create LLM service instance"""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service
