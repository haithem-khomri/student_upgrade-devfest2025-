# Gemini API Integration Guide

## Overview

The chatbot now uses **real Google Gemini API calls** to generate AI responses based on user questions. This document explains how the integration works and how to verify it's functioning correctly.

## How It Works

### Backend Flow

1. **User sends a message** → Frontend calls `/api/chat` or `/api/v1/chatbot/chat`
2. **Backend receives request** → Routes to `public_chat.py` or `chatbot.py`
3. **LLM Service called** → `LLMService.chat_completion()` is invoked
4. **Gemini API called** → `GoogleGeminiProvider.complete()` makes actual API call to Google
5. **Response returned** → AI-generated response sent back to frontend

### Key Components

- **`backend/app/services/ai/llm_service.py`**: Main LLM service that routes to Gemini
- **`GoogleGeminiProvider`**: Handles actual Gemini API calls
- **`backend/app/core/config.py`**: Configuration for API keys and model selection

## Configuration

### Environment Variables

Set these environment variables (or in `.env` file):

```env
LLM_PROVIDER=google
GOOGLE_API_KEY=your-api-key-here
GOOGLE_MODEL=gemini-1.5-flash
```

### Using PowerShell Script

The `backend/start_server.ps1` script automatically sets these:

```powershell
$env:LLM_PROVIDER = "google"
$env:GOOGLE_API_KEY = "your-api-key"
```

### Using .env File

Create `backend/.env`:

```env
LLM_PROVIDER=google
GOOGLE_API_KEY=your-api-key-here
GOOGLE_MODEL=gemini-1.5-flash
MONGODB_URL=your-mongodb-url
MONGODB_DB_NAME=student_ai
SECRET_KEY=your-secret-key
```

## Verification

### 1. Check Health Endpoint

Visit: `http://localhost:8000/health`

You should see:

```json
{
  "status": "healthy",
  "mongodb": "connected",
  "database": "student_ai",
  "llm": {
    "provider": "google",
    "configured": true,
    "api_key_set": true,
    "model": "gemini-1.5-flash"
  }
}
```

### 2. Run Test Script

```bash
cd backend
python test_gemini.py
```

This will:
- Check configuration
- Initialize LLM service
- Make a test API call to Gemini
- Display the response

Expected output:
```
✅ SUCCESS! Received response from Gemini:
------------------------------------------------------------
[AI-generated response in Arabic]
------------------------------------------------------------
```

### 3. Check Server Logs

When you send a chat message, you should see logs like:

```
[LLMService] Initializing Google Gemini provider with model: gemini-1.5-flash
[Gemini] Configured with model: gemini-1.5-flash
[LLMService] Calling google provider for message: مرحبا...
[Gemini] Calling API with model: gemini-1.5-flash, message length: 15
[Gemini] Successfully received response (length: 234)
[LLMService] Successfully received response from google (length: 234)
```

### 4. Test via API

**Public Chat (No Auth Required):**

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "مرحبا، كيف حالك؟", "language": "ar"}'
```

**Authenticated Chatbot:**

```bash
curl -X POST http://localhost:8000/api/v1/chatbot/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"message": "مرحبا", "language": "ar"}'
```

## Troubleshooting

### Issue: "No LLM provider configured"

**Solution:**
- Check `LLM_PROVIDER` is set to `"google"` (not `"none"`)
- Verify `GOOGLE_API_KEY` is set
- Check server logs for initialization messages

### Issue: "Google API error: API key not valid"

**Solution:**
- Verify your API key is correct
- Get a new key from [Google AI Studio](https://makersuite.google.com/app/apikey)
- Make sure the key has Gemini API access enabled

### Issue: "Model not available"

**Solution:**
- The code automatically falls back to `gemini-1.5-flash` if the configured model doesn't exist
- Check available models: `gemini-1.5-flash`, `gemini-1.5-pro`, `gemini-pro`

### Issue: "Falling back to predefined responses"

**Solution:**
- Check server logs for the actual error
- Verify internet connection
- Check if API key has quota/rate limits
- Ensure `google-generativeai` package is installed: `pip install google-generativeai`

### Issue: No logs appearing

**Solution:**
- Make sure the backend server is running
- Check that you're calling the correct endpoint
- Verify the frontend is pointing to the right backend URL (`http://localhost:8000`)

## Features

### ✅ Real AI Responses
- All chatbot responses are generated by Google Gemini
- Responses are contextual and relevant to the question

### ✅ Error Handling
- Falls back to predefined responses if API fails
- Logs all errors for debugging
- Graceful degradation

### ✅ Logging
- Detailed logs show when Gemini is called
- Response lengths and timing information
- Error messages with context

### ✅ Model Configuration
- Configurable model selection
- Automatic fallback to compatible models
- Support for different Gemini models

## API Endpoints

### Public Chat
- **Endpoint**: `POST /api/chat`
- **Auth**: Not required
- **Use**: Landing page quick questions

### Authenticated Chatbot
- **Endpoint**: `POST /api/v1/chatbot/chat`
- **Auth**: Required (Bearer token)
- **Use**: Full chatbot with user context

### Health Check
- **Endpoint**: `GET /health`
- **Auth**: Not required
- **Use**: Verify LLM configuration

## Next Steps

1. **Get API Key**: Sign up at [Google AI Studio](https://makersuite.google.com/app/apikey)
2. **Configure**: Set environment variables or use `.env` file
3. **Test**: Run `python backend/test_gemini.py`
4. **Verify**: Check `/health` endpoint
5. **Use**: Start chatting and check server logs!

## Support

If you encounter issues:
1. Check server logs for detailed error messages
2. Run the test script: `python backend/test_gemini.py`
3. Verify configuration at `/health` endpoint
4. Check that `google-generativeai` package is installed

